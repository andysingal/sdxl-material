```
# 创建conda环境
% conda create -n sd python=3.10
% conda activate sd

# 拉取训练代码
% git clone git@github.com:kohya-ss/sd-scripts.git
% cd sd-script
% export KOHYA_SS_PATH=$PWD

# 创建/激活venv环境
% python3 -m venv venv/
% source venv/bin/activate

# 安装依赖包
(venv)(sd)% pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 torchtext==0.15.2 torchdata==0.6.1 --extra-index-url https://download.pytorch.org/whl/cu118 
(venv)(sd)% pip install xformers==0.0.20 triton==2.0.0 gradio_client==0.2.7
(venv)(sd)% pip install -r requirements.txt

```
0x02 Prepare training set
Let's first play a video about the principles of training set selection. This video is very good.

https://www.youtube.com/watch?v=j-So4VYTL98 _ _

If you want to train very complex images, such as architecture or art types, and each image is different, you need as many as 100 samples.

If you are just training faces and the style does not change much, you can also have 15 high-definition sample pictures.

Almost all Chinese tutorials allow you to crop to 512x512 or 768x768, but this step is not necessary, as long as the length and width of the sample image are between the -- min_bucket_reso and -- max_bucket_reso parameters specified when running the training script . These two values ​​are generally set to 256 and 1024

training image collection
Today I am training Yuya Mikami's face, and the training set can be downloaded from her little blue bird and Instagram. The principle of selecting the training set is: the background is clean, the face is not blocked, the photos of the big head and the half body are the majority, and a small part of the whole body is taken . You can download more photos when downloading, and then use them as regularized image sets.

```
(venv)(sd)% wget https://raw.githubusercontent.com/er1cw00/stable-diffusion-script/dev/utils/image_rename.sh
(venv)(sd)% wget https://raw.githubusercontent.com/er1cw00/stable-diffusion-script/dev/utils/image_resize.sh
(venv)(sd)% chmod a+x image_rename.sh image_resize.sh 

# 重命名照片文件
(venv)(sd)% ./image_rename.sh ~/MikamiYua/ins/ MikamiYua
rename all images in /Users/er1cw00/MikamiYua/ins >>>>
transfor 298184208_2847242105582604_1212075565102317429_n.jpg -> MikamiYua_1.jpg
transfor 298320441_736337670779130_6566987790894100116_n.jpg -> MikamiYua_2.jpg
transfor 301449702_484488646832243_8012007167960806349_n.jpg -> MikamiYua_3.jpg
...

#保持长宽比例缩放长边<1024
(venv)(sd)% ./image_rename.sh ~/MikamiYua/ins/ ~/MikamiYua/dataset
1024x1024>
resize MikamiYua_1.jpg
resize MikamiYua_2.jpg
resize MikamiYua_3.jpg
```
After entering the tab page of the Tagger plug-in, both "Input directory" and "output directory" are set to the image directory preprocessed in the previous step. The generated tag files and image files are put together, and "Remove duplicated tag" is checked. Threshold" also remains unchanged at 0.35.

In the "Additional tags" we fill in Mr. Mikami's English name Mikamiyua as the trigger word, and in the "Exclude tags" we fill in lips, eyes, hair and other face-related prompt words. For LoRA training, if the inverted label is deleted, the features of this label will be integrated into the trigger words.


Regularized image set
Regularizing the image set is to prevent language drift. My personal understanding is that when training Teacher Mikami, the characteristics of the training set photos biased the characteristics of 1girl (the entire class from being pulled by the learning target). The regularized image set is an optional training set, and it is not necessary to provide back-inferred labels for the regularized image set. A regularized image set can be a larger number of training images of the same type, which are also trained, so their quality affects the model.

0x03 start training
Gallery configuration
The configuration file dataset.toml of the training image set , the [datasets] section contains two subsets pointing to the training image set directory and the regularized image set directory respectively. If the regularized image set is not included, delete the "is_reg = true" subsets. Can:

shuffle_caption: Randomly shuffle the labels during training, which can improve the generalization ability of the model

keep_tokens: Keep the first n positions of the label unchanged,

flip_aug: Flip the training images left and right

enable_bucket: Enable bucketing. When the images in the training set are inconsistent in size, they will be assigned to batches of different sizes according to the image size.

bucket_no_upscale: Do not enlarge the image when bucketing

bucket_reso_steps: Bucket compensation, the default value is 64

min_bucket_reso/max_bucket_reso: minimum/maximum resolution

num_repeats: Specifies the number of times each round of training for images in the specified image set

image_dir: Specifies the path to the image of the image set

class_tokens: Specifies the image set identifier class

is_reg: Specifies the image set as a regularized image set

```
[general]
resolution = 512
shuffle_caption = true 
keep_tokens = 1
flip_aug = false
caption_extension = ".txt"
enable_bucket = true
bucket_reso_steps = 64
bucket_no_upscale = false
min_bucket_reso = 256
max_bucket_reso = 1024

[[datasets]]

  [[datasets.subsets]]
  num_repeats = 20
  image_dir = "/Users/er1cw00/LoRA/MikamiYua/dataset"

  [[datasets.subsets]]
  is_reg = true
  class_tokens = "1girl"
  num_repeats = 1
  image_dir = "/Users/er1cw00/LoRA/MikamiYua/regularization"
```

Training parameter configuration
unet_lr/text_encoder_lr: learning rate of unet/text_encoder

network_dim/network_alpha: network_dim specifies the rank of the LoRA model, alpha is less than the dim value, and is generally set to half of dim. network_dim determines the size of the LoRA model, and the 144MB LoRA model corresponds to network_dim=128.

network_module: Specify the training LoRA model

max_train_epochs: Specifies the maximum number of training epochs

save_every_n_epochs: Save the model every n rounds

save_last_n_epochs: Save the model for the last n rounds

train_batch_size: Specifies the parallelism of training

save_state: Whether to save the intermediate state of the training, if this switch is enabled, when the training is interrupted, the next time the training is restarted, the current training can be continued through --resume

clip_skip: Specify 2 to use the output of the penultimate layer of the text encoder (CLIP). If omitted or 1 specified, the last layer is used

min_snr_gamma: Specify the minimum SNR weighting strategy .

max_token_length: The default value is 75. Specify "150" or "225" to extend the label length

save_precision/mixed_precision: respectively specify the precision of the saved model and the mixed precision during training. There is no mixed precision by default. Specifying fp16/bf16 can significantly reduce the video memory and improve the training speed.

output_dir/logging_dir: specify the directories for saving model files and output log files respectively

output_name/log_prefix: Specify the file name prefix for saving the model file and output log file respectively.

pretrained_model_name_or_path: Specify the base model

cache_latents: Cache VAE output in main memory to reduce VRAM usage, which can improve training speed

```
[additional_network_arguments]
unet_lr = 0.0005
text_encoder_lr = 0.0001
network_dim = 64
network_alpha = 32
network_module = "networks.lora"

[optimizer_arguments]
learning_rate = 0.0005
lr_scheduler = "cosine_with_restarts"
lr_scheduler_num_cycles = 3
lr_warmup_steps = 82
optimizer_type = "AdamW8bit"

[training_arguments]
max_train_epochs = 10
save_every_n_epochs = 1
save_last_n_epochs = 6
clip_skip = 2
min_snr_gamma = 5.0
weighted_captions = false
seed = 42
max_token_length = 75
xformers = true
lowram = true
max_data_loader_n_workers = 8
persistent_data_loader_workers = true
save_precision = "fp16"
mixed_precision = "fp16"
output_dir = "/Users/er1cw00/LoRA/MikamiYua/output"
logging_dir = "/Users/er1cw00/LoRA/MikamiYua/logs"
output_name = "MikamiYua"
log_prefix = "MikamiYua"
save_state = false

[model_arguments]
pretrained_model_name_or_path = "/Users/er1cw00/stable-diffusion-webui/models/Stable-diffusion/majicmixRealistic_v6.safetensors"
v2 = false

[saving_arguments]
save_model_as = "safetensors"

[dreambooth_arguments]
prior_loss_weight = 1.0

[dataset_arguments]
cache_latents = true
```
1. https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md
2. https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_README-en.md
3. https://github.com/kohya-ss/sd-scripts/blob/main/docs/train_README-zh.md


